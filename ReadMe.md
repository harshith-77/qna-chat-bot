# Q&A Chatbot

## ğŸ“Œ Overview
Q&A chatbot is an AI-powered chatbot that answers your questions quickly by retrieving question and answer pairs from the uploaded **CSV file**.  
The implementation leverages **Retrieval-Augmented Generation (RAG)** to:
1. **Retrieve relevant Q&A pairs from the uploaded CSV**.
2. **Reranks the retrieved results** to select the most relevant context.
3. **Pass the final context to an LLM (Large Language Model)** for accurate responses.

## ğŸš€ Features
âœ… Upload a CSV file containing questions and answers.  
âœ… Store and retrieve data using **FAISS Vector Database**.  
âœ… **Rerank** the retrieved data to select the most relevant data.  
âœ… Use **Google Generative AI** for intelligent responses.  
âœ… **Streamlit UI** for an interactive chatbot experience.  
âœ… **Efficient session handling** to avoid unnecessary vector database recreation.

---

## ğŸš€ Reranker Model Integration

### **ğŸ”¹ What is a Reranker?**
A **reranker** is an advanced model that **reorders** retrieved documents based on their relevance to the query. Traditional RAG (Retrieval-Augmented Generation) methods rely on similarity-based retrieval (e.g., FAISS), which may return results that are **somewhat relevant but not necessarily the best**.

A **reranker model** takes the initially retrieved documents and **scores them again** with a deeper understanding of context. This ensures that the **top-ranked documents are the most relevant**, improving the quality of answers generated by the LLM.

### **ğŸ”¹ Why Use a Reranker in RAG?**
Traditional FAISS-based retrieval **relies on dense embeddings** to find similar documents. However, embeddings may **miss nuances** in language, leading to slightly off-topic results.

A reranker **fixes this** by:  
- Understanding **semantic meaning** rather than just vector similarity.  
- **Reordering results** to push the **most relevant** ones to the top.  
- Reducing **hallucinations** by providing the LLM with **high-quality context**.

### ğŸ”¹ **Advantages of Reranker over Traditional RAG**
- **Better Relevance** â€“ Ensures only the **most useful** documents are passed to the LLM.  
- **Reduced Hallucinations** â€“ LLM bases its answers on **higher-quality context**.  
- **Improved Query Understanding** â€“ Handles **ambiguous queries** better than FAISS alone.  
- **Works Well with Long Documents** â€“ Identifies **key passages** instead of relying on surface similarity.  

---

## ğŸ› ï¸ Installation & Setup

### 1. **Clone the Repository**
```bash
https://github.com/harshith-77/qna-chat-bot.git
cd your-repo
```

### 2. **Create a Virtual Environment (Optional but Recommended)**
```bash
python -m venv venv
source venv/bin/activate  # On macOS/Linux
venv\Scripts\activate  # On Windows
```

### 3. **Install Dependencies**
```bash
pip install -r requirements.txt
```

### 4. **Set Up API Keys**
- Create a .env file in the project directory.
- Add your Google Gemini API Key:
```bash
GEMINI_API_KEY=your_google_gemini_api_key
```

### 5. **Run the Chatbot**
```bash
streamlit run main.py
```

---

## âš™ï¸ Implementation Details

### **1ï¸âƒ£ CSV File Upload**
- Users upload a **CSV file** containing Q&A pairs.
- The file is **saved locally** in an `uploaded_files/` directory.
- If a file with the same name is uploaded again, it is **not reprocessed** to save resources.

### **2ï¸âƒ£ Vector Database Creation (FAISS)**
- The CSV is converted into **text embeddings** using `HuggingFaceEmbeddings`.
- A **FAISS vector store** is created to efficiently **retrieve similar Q&A pairs**.

### **3ï¸âƒ£ Query Processing & RAG Flow**
1. The user enters a question in the chatbot.
2. The **retriever searches for similar Q&A pairs** in the FAISS database and fetches top **k** elements.
3. The retrieved documents are **passed through a reranker model**. The **5 most relevant** documents are selected.
4. The reranked results are **passed to the LLM (`gemini-2.0-flash-exp`)**.
5. The LLM generates a response **only based on the given context**.
6. The chatbot returns the **final answer**.


---

## ğŸ—ï¸ Tech Stack
- **Python** ğŸ  
- **Streamlit** ğŸ›ï¸ (Frontend UI)  
- **FAISS** ğŸ” (Vector Search)  
- **HuggingFaceCrossEncoder** â¬†ï¸ (Re-ranking)
- **HuggingFace Embeddings** ğŸ§  (Text Embeddings)  
- **Google Generative AI (Gemini 2.0)** ğŸ¤– (LLM)  
- **LangChain** ğŸ—ï¸ (RAG Pipeline)

---

## ğŸ”¥ Potential Improvements

- **Caching for Faster Retrieval:** We can cache the FAISS index and embeddings. This prevents recomputation when using the same dataset.

- **Persistent Vector Store:** Store the **FAISS index on disk** to avoid reprocessing on every restart. 

- **Support for Multiple CSV Files:** Allow users to upload and switch between multiple CSV files dynamically.
